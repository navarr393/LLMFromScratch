- Code for book "Build a Large Language Model (From Scratch)"

Important Vocab:
- Byte pair encoding: A more sphisticated tokenization shceme based on the concept called byte pair encoding, used in GPT-2, GPT-3 and the original ChatGPT.
- BPE tokenizers break down unknown words into subwords and idividual characters. This way a BPE tokenizer can parse any word and doesnt need to replace unknown words with special tokens, such as '<|unk|>'